[
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Tools and utilities",
    "section": "",
    "text": "Notebooks and Git\nJupyter notebooks are technically JSON files with possibly embedded binary data in output cells (e.g. images). This make them not very Git friendly, because most Git tools are designed to work with plain text files. Git diffs of notebooks are not very readable, merges break notebooks, and binary blobs clog storage and don’t diff or merge. Multiple approaches exist to address these problems, and I recommend using the Jupytext tool to only version plaintext replicas of notebooks and add .ipynb files to .gitignore.\n\n\nNBD: development in the notebook\nThis section defines the Nbd class that can be used to convert a notebook into a script or a documentation page. This approach was greatly inspired by the nbdev project and can be thought of as a reduced and simplified nbdev.\n\n\n\nnbd cells\n\n\nTo use Nbd, create an instance with the name of your package and call methods from that instance like so: nbd = Nbd('popemp').\nMethod Nbd.nb2mod() selectively exports code cells into a script, making it easily importable in other parts of the project and also leaving out scratch and exploratory code. To mark code cell for export to a module, give it a nbd-module tag. All imports from project modules into notebooks should take absolute form from ... import ..., they will be automatically converted to relative import in scripts. For example, from popemp.tools import Nbd will become from .tools import Nbd.\nExample and testing of Nbd root dir finder.\n\n\nCode\nnbd = Nbd('popemp')\nprint(f'Project root directory: \"{nbd.root}\"')\n\n\nProject root directory: \"/Users/anton/work/workshop-notebooks\"\n\n\n\n\nDocumentation\nStatic HTML documentation website is built from notebooks using Quarto. filter_docs() function is used to only select cells with nbd-docs tag.\n\n\nMisc\ndownload_file() takes URL, downloads file to specified location and returns it’s path. Subsequent calls to the same function return cached copy from disk. We use this function to automate manual operations, to simplify replication and to allow pulling data into our cloud-hosted dashboard.\nExample: download LICENSE file from GitHub repo and assert that it’s contents is the same as in the local repo version.\n\n\nCode\nnbd = Nbd('popemp')\nf = download_file('https://raw.githubusercontent.com/antonbabkin/workshop-notebooks/main/LICENSE', nbd.root, 'LICENSE_COPY')\nassert open(nbd.root/'LICENSE').read() == open(f).read()\nf.unlink()\n\n\nDownload complete: LICENSE_COPY.\n\n\n\n\nCLI interface\nIf tools.py is execuded directly as a module with filter-docs argument, it will apply the documentation filter. This is used for ipynb-filters option of Quarto renderer.\n\n\nBuild this module\nThis notebook itself is turned into importable module by running the code below.\n\n\nCode\nnbd = Nbd('popemp')\nnbd.nb2mod('tools.ipynb')\n\n\nConverted notebook \"nbs/tools.ipynb\" to module \"popemp/tools.py\"."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Retrieve and prepare data",
    "section": "",
    "text": "Geography\nWe need state and county FIPS codes and names, and their shapes for map visualizations. Here we use 2018 Cartographic Boundary Files - simplified representations of selected geographic areas from the Census Bureau’s MAF/TIGER geographic database. These boundary files are specifically designed for small scale thematic mapping.\nFunction geo() downloads state and county 1:20,000,000 shapefiles using geopandas library, reshapes and combines them into single a GeoDataFrame. We use county code \"000\" as indicator of state rows. Resulting dataframe is cached on disk as a binary pickle file, and when subsequent calls of geo() will simply read and return the dataframe from cache to save time and avoid work. Delete data/geo.pkl if you want to re-create the dataframe, for example, after you changed the function. Similarly, download_file() also caches files on disk.\nThis is the top of the dataframe.\n\n\nCode\ngeo().head()\n\n\n\n\n\n\n  \n    \n      \n      st\n      cty\n      name\n      geometry\n    \n  \n  \n    \n      0\n      01\n      000\n      Alabama\n      POLYGON ((-88.46866 31.89386, -88.46866 31.933...\n    \n    \n      1\n      01\n      001\n      Autauga county, Alabama\n      POLYGON ((-86.91759 32.66417, -86.71339 32.661...\n    \n    \n      2\n      01\n      003\n      Baldwin county, Alabama\n      POLYGON ((-88.02632 30.75336, -87.94455 30.827...\n    \n    \n      3\n      01\n      005\n      Barbour county, Alabama\n      POLYGON ((-85.73573 31.62449, -85.66565 31.786...\n    \n    \n      4\n      01\n      007\n      Bibb county, Alabama\n      POLYGON ((-87.42194 33.00338, -87.31854 33.006...\n    \n  \n\n\n\n\ngeopandas stores shapes as shapely polygons in the geometry column. You can perform various geometric operations with these objects, refer to geopandas and shapely documentation. For example, let’s select and plot all states that cross the band between -120 and -110 degrees of longitude, roughly US Pacific coast.\n\n\nCode\nd = geo().cx[-120:-110, :].query('cty == \"000\"')\nd.plot();\n\n\n\n\n\nBe mindful of Coordinate Reference System (CRS) when working with shapefiles. If you combine shapefiles from multiple sources, make sure to align their CRS’s. Census shapefiles come in EPSG:4269. The same map in “Spherical Mercator” (EPSG:3857, used in Google Maps) will look like this.\n\n\nCode\nd.to_crs(epsg=3857).plot();\n\n\n\n\n\n\n\nPopulation\nWe are using annual state and county population 1990-2019 from Census Population Estimates Program (PEP). Data are available in 10 year blocks for 2010-2019, 2000-2010 and 1990-1999.\nNote on character encoding of plain text files, including CSV: newer files use \"UTF-8\", older use \"ISO-8859-1\".\nPost-2000 files are simple CSV tables. Functions pop_2010_2019() and pop_2000_2009() download and read them into dataframes with minor manipulation.\n1990-1999 data are in a long text file. pop_1990_1999() does some more elaborate parsing. Table with state and county population has \"1\" as the first character in every line. We use this to read necessary lines into a temporary string buffer, and then parse the buffer into a dataframe.\nFinally, in pop() we call the three above functions to create three frames, combine them and add aggregated rows of national totals with state code \"00\" and county code \"000\". We also compute year-to-year growth rate in percentage points in column pop_gr. Final dataframe is pickled for easy access.\n\n\nCode\npop().head()\n\n\n\n\n\n\n  \n    \n      \n      st\n      cty\n      year\n      pop\n      pop_gr\n    \n  \n  \n    \n      0\n      00\n      000\n      1990\n      249470539\n      NaN\n    \n    \n      1\n      00\n      000\n      1991\n      252208537\n      1.097524\n    \n    \n      2\n      00\n      000\n      1992\n      255104027\n      1.148054\n    \n    \n      3\n      00\n      000\n      1993\n      257857622\n      1.079401\n    \n    \n      4\n      00\n      000\n      1994\n      260401091\n      0.986385\n    \n  \n\n\n\n\nQuick visual inspection of the data reveals an abnormal population jump between 1999 and 2000. It is clear on national and state level, but not so on county level. I could not find out the cause, but it is most likely a data artifact. This is something to be aware of, but it does not matter for the purposes of this project.\n\n\nCode\nd = pop().set_index('year')\nfig, ax = plt.subplots(1, 3, figsize=(16, 4))\nd.query('st == \"00\" and cty == \"000\"')['pop'].plot(ax=ax[0])\nax[0].set_title('National')\nd.query('st == \"55\" and cty == \"000\"')['pop'].plot(ax=ax[1])\nax[1].set_title('Wisconsin')\nd.query('st == \"55\" and cty == \"025\"')['pop'].plot(ax=ax[2])\nax[2].set_title('Wisconsin, Dane county');\n\n\n\n\n\n\n\nEmployment\nState and county employment comes from Census Business Dynamics Statistics (BDS). This product has some improvements over more widely used County Business Patterns, and entire history can be downloaded in a single table from here.\nData does not require much processing which is done in the emp(). National, state and county tables are downloaded and combined, again using convention of setting state to \"00\" for national and county to \"000\" for national and state rows. Percentage year-to-year growth rate is renamed from net_job_creation_rate to emp_gr. Data goes back to 1978, but we only need from 1990 for combination with population.\n\n\nCode\nemp().head()\n\n\n\n\n\n\n  \n    \n      \n      st\n      cty\n      year\n      emp\n      emp_gr\n    \n  \n  \n    \n      0\n      00\n      000\n      1990\n      93983875.0\n      2.145\n    \n    \n      1\n      00\n      000\n      1991\n      91781210.0\n      -2.439\n    \n    \n      2\n      00\n      000\n      1992\n      91752935.0\n      0.004\n    \n    \n      3\n      00\n      000\n      1993\n      93252746.0\n      1.560\n    \n    \n      4\n      00\n      000\n      1994\n      95712240.0\n      2.254\n    \n  \n\n\n\n\n\n\nCode\nd = emp().set_index('year')\nfig, ax = plt.subplots(1, 3, figsize=(16, 4))\nd.query('st == \"00\" and cty == \"000\"')['emp'].plot(ax=ax[0])\nax[0].set_title('National')\nd.query('st == \"55\" and cty == \"000\"')['emp'].plot(ax=ax[1])\nax[1].set_title('Wisconsin')\nd.query('st == \"55\" and cty == \"025\"')['emp'].plot(ax=ax[2])\nax[2].set_title('Wisconsin, Dane county');\n\n\n\n\n\n\n\nAPI\nHere is a little demo of retrieving a table from a data provider using API. We are not going to use it in this project, because bulk data download as readily available as CSV files, and API access rates are often limited and may require access key. However for some other data sources API access may be the only option. Another good use case is when whole data is huge, and you are building a web app (dashboard) that only needs to pull small pieces of data at a time.\nHere I show how to query a portion of the BDS dataset from Census Bureau API. Most of Census data products can be accessed like this, and BDS specific documentation is here with some query examples there.\nTypically, to use an API you need to submit a HTTP request and receive back a response. Request queries are customized by chanding parameters of the URL string, and responses return data in JSON, XML or some other format. Python requests library hides a lot of technical details and is easy to use. When you are constructing your query URL, you can also just open it in a browser for a quick preview.\nHere is a query line that will pull employment data for all counties in Wisconsin from 2015 to 2019.\nhttps://api.census.gov/data/timeseries/bds?get=NAME,ESTAB,EMP,YEAR&for=county:*&in=state:55&time=from+2015+to+2019&NAICS=00&key=YOUR_KEY_GOES_HERE\nEverything to the left of ? is the base URL or endpoint: https://api.census.gov/data/timeseries/bds.\nEverything to the right are key-value parameter pairs, separated by &:\nget=NAME,ESTAB,EMP,YEAR data columns\nfor=county:* all counties\nin=state:55 state FIPS code “55” for Wisconsin\ntime=from+2015+to+2019 time series range\nNAICS=00 “00” for economy-wide employment\nkey=YOUR_KEY_GOES_HERE drop this part if you don’t have a key\nQuery limits: > You can include up to 50 variables in a single API query and can make up to 500 queries per IP address per day. More than 500 queries per IP address per day requires that you register for a Census key. That key will be part of your data request URL string.\nQuerying without a key will probably work for you, unless you are sharing your IP with many other users. You can obtain a key for free, but you should keep it secret and not accidentally share, for example, by hard-coding it in your code or commiting a file. Here I have my key in a text file that is ignored in .gitignore and only exists in my local copy of the repo. Another common appoach is to store keys in OS environment variables.\n\n\nCode\nimport requests\n\np = nbd.root/'census_api_key.txt'\nif p.exists():\n    key = '&key=' + p.read_text()\nelse:\n    key = ''\n\nbase_url = 'https://api.census.gov/data/timeseries/bds'\nst = '55'\ny0, y1 = 2015, 2019\nresponse = requests.get(f'{base_url}?get=NAME,ESTAB,EMP,YEAR&for=county:*&in=state:{st}&time=from+{y0}+to+{y1}&NAICS=00{key}')\nresponse_body = response.json()\nresponse_body[:5]\n\n\n[['NAME', 'ESTAB', 'EMP', 'YEAR', 'time', 'NAICS', 'state', 'county'],\n ['Iron County, Wisconsin', '176', '1360', '2015', '2015', '00', '55', '051'],\n ['Iron County, Wisconsin', '172', '1416', '2016', '2016', '00', '55', '051'],\n ['Iron County, Wisconsin', '172', '1337', '2017', '2017', '00', '55', '051'],\n ['Iron County, Wisconsin', '166', '1362', '2018', '2018', '00', '55', '051']]\n\n\n\n\nCode\ndf = pd.DataFrame(response_body[1:], columns=response_body[0])\ndf.query('county == \"025\"').head()\n\n\n\n\n\n\n  \n    \n      \n      NAME\n      ESTAB\n      EMP\n      YEAR\n      time\n      NAICS\n      state\n      county\n    \n  \n  \n    \n      246\n      Dane County, Wisconsin\n      12649\n      269895\n      2015\n      2015\n      00\n      55\n      025\n    \n    \n      247\n      Dane County, Wisconsin\n      12974\n      275642\n      2016\n      2016\n      00\n      55\n      025\n    \n    \n      248\n      Dane County, Wisconsin\n      13047\n      280403\n      2017\n      2017\n      00\n      55\n      025\n    \n    \n      249\n      Dane County, Wisconsin\n      13088\n      297744\n      2018\n      2018\n      00\n      55\n      025\n    \n    \n      250\n      Dane County, Wisconsin\n      13099\n      301777\n      2019\n      2019\n      00\n      55\n      025"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis of population and employment dynamics",
    "section": "",
    "text": "In this module we will combine economic, demographic and geographic data to explore patterns of population and employment dynamics across states and counties.\nMain dataframes are stored in a global dict DF. During interactive notebook execution it is populated as needed. If imported as a module, function prep_data() should be called before using other module functions.\nIn data_by_year() we simply merge employment and population dataframes available from popemp.data module."
  },
  {
    "objectID": "analysis.html#widgets",
    "href": "analysis.html#widgets",
    "title": "Analysis of population and employment dynamics",
    "section": "Widgets",
    "text": "Widgets\nJupyter widgets are like other Python objects such as strings, lists or pandas dataframes. Like other objects widgets also store their state, have methods to do something useful with that state and have a representation suitable for rich rendering in a HTML view of a Jupyter notebook. Additional feature of widgets is that their visual representation can be updated dynamically and they can respond to user interaction.\nHere is a simple slider. We can read it’s value in code from another cell and also change it’s value programmatically.\n\n\nCode\nw = widgets.IntSlider(value=4, min=0, max=10, description='How many?')\nw\n\n\n\n\n\n\n\nCode\nprint('He says', w.value)\n\n\nHe says 4\n\n\n\n\nCode\nw.value = 5\n\n\n\n\nCode\nprint('Now he says', w.value)\n\n\nNow he says 5\n\n\nWe can combine multiple widgets and make them do something useful together. A button here will add up two numbers and display result in a separate label widget. We can even be fancy and use \\(\\LaTeX\\) in text labels.\n\n\nCode\n# create widgets\nwx = widgets.IntSlider(2, 0, 5, description='$x$')\nwy = widgets.IntSlider(2, 0, 5, description='$y$')\nwb = widgets.Button(description='Add')\nwz = widgets.Label('$x + y = ?$')\n\n# \"useful\" function\ndef how_many(x, y):\n    z = x + y\n    return 5\n\ndef click_handler(*args):\n    # \"*args\" captures arguments passed from calling widget, but we ignore them here\n    x = wx.value\n    y = wy.value\n    z = how_many(x, y)\n    wz.value = f'${x} + {y} = {z}$'\n# run handler to fill initial values\nclick_handler()\n# register handler with button widget\nwb.on_click(click_handler)\n\n# display widgets in a simple vertical layout\nwidgets.VBox([wx, wy, wb, wz])\n\n\n\n\n\nFunction st_cty_selectors() creates two dropdown widgets that can be used to select state and county using their names instead of codes, while codes are used internally to work with our dataframes. Lists of states and counties are populated from our global tables. Additional logic, wrapping inside of the function, updates list of counties dynamically every time the state is changed. We can now create a pair of linked widgets anywhere we need them later."
  },
  {
    "objectID": "analysis.html#static-interactivity-with-plotly",
    "href": "analysis.html#static-interactivity-with-plotly",
    "title": "Analysis of population and employment dynamics",
    "section": "Static interactivity with Plotly",
    "text": "Static interactivity with Plotly\nOne big downside of widgets is that they require a running Jupyter kernel to fully function. This means that interactivity will be unavailable if you share your notebook as a static HTML page, and you need some kind of server solution such as Binder to share your interactive reports. Some tools, however, generate interactive output with all dependencies and data embedded in the HTML page itself, and can thus be hosted as a static page. Plotly is one such tool that can be used to create figures that users can interact with.\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = 'plotly_mimetype+notebook_connected'\n\ndf = compute_agr(2005, 2015).query('st == \"55\"')\ndf = df.merge(pd.DataFrame(DF['geo'].query('st == \"55\"')[['st', 'cty', 'name']]), 'left')\n\nfig = px.scatter(df, x='pop_agr_rel', y='emp_agr_rel', color='agr_cat_rel', hover_name='name', height=600, width=600)\nfig.update_layout()\nfig.show()"
  },
  {
    "objectID": "analysis.html#folium",
    "href": "analysis.html#folium",
    "title": "Analysis of population and employment dynamics",
    "section": "folium",
    "text": "folium\nPython package folium is a wrapper around a popular JavaScript map visualization library Leaflet.js. Maps created with folium are static and can be included into HTML documentation for some degree of interactivity. folium is conveniently exposed in geopandas.GeoDataFrame.explore() method.\n\n\nCode\ndf = area_gdf('55', 2005, 2015, 'abs')\ndf.explore(color='color')\n\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "analysis.html#ipyleaflet",
    "href": "analysis.html#ipyleaflet",
    "title": "Analysis of population and employment dynamics",
    "section": "ipyleaflet",
    "text": "ipyleaflet\nPython package ipyleaflet is also a Leaflet.js wrapper. The main difference from folium is that map objects are Jupyter widgets that can be used for bidirectional interactivity and also combined with all other widgets.\nHere we wrap a map widget in a class Map that stores map state and exposes interaction via click_callback and upd() methods."
  }
]