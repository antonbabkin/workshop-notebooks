{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Literate programming and interactive reporting with Jupyter notebooks | Documentation | Dashboard Workshop materials for the 2022 Data Science Research Bazaar at UW-Madison. This is an intermediate level workshop that will teach how to use Jupyter notebooks with Python for literate programming and interactive presentation of research results. I walk participants through steps of a mini research project of visualizing geospatial data, focusing on tools and methods of making results accessible, reproducible and interactive. This workshop will use economic, demographic and geographic data characterizing US communities that are freely and publicly available from the US Census Bureau website. Prior programming and data analysis experience is recommended for full participation in the workshop. If you are new to Python, I recommend reading about Jupyter and pandas . This book shows how to use Jupyter notebooks for teaching and learning, and QuantEcon lectures use Python for economics and finance and are also a good resource for beginners. Workshop topics: - Jupyter notebooks and Markdown - code organization, Python modules and packages, conversion to scripts - static documentation, conversion to HTML - data retrieval via download and API - data processing and analysis in pandas - plotting with matplotlib - building interactive interfaces with notebook widgets - visualization of geospatial data, geopandas and ipyleaflet - hosting live notebooks and Voil\u00e0 dashboards on Binder Setup You need a running Jupyter server in order to work with workshop notebooks. The easiest way is to launch a free cloud instance in Binder . A more difficult (but potentially more reliable) alternative is to create conda Python environment on your local computer. Using Binder Click this link to launch a new Binder instance and connect to it from your browser, then open and run the init notebook to test the environment and initialize paths. Ideal launch time is under 30 seconds, but it might take longer if the repository has been recently updated, because Binder will need to rebuild the environment from scratch. Notice that Binder platform provides computational resources for free, and so limitations are in place and availability can not be guaranteed. Read here about usage policy and available resources. Also static HTML documentation site will not be available. Local Python This method requires some experience or readiness to read documentation. As reward, you will have persistent environment under your control that does not depend on cloud service availability. This is also a typical way to set up Python for data work. Download and install latest miniconda , following instructions for your operating system. Skip this step if you already have conda command line utility available in your terminal. Open terminal (Anaconda Prompt on Windows) and clone this repository in a folder of your choice ( git clone https://github.com/antonbabkin/workshop-notebooks.git ). Alternatively, download and unpack repository code as ZIP. If you want to practice building and hosting HTML documentation, you need to use a repository that you control. One way to do it is to fork this repository and then clone your copy. In the terminal, navigate to the repository folder and create new conda environment . Environment specification will be read from the environment.yml file, all required packages will be downloaded and installed. cd workshop-notebooks conda env create Activate the environment and start JupyterLab server. This will start a new Jupyter server and open Jupyter interface in browser window. conda activate workshop-nbs-2022 jupyter lab In Jupyter, open and run the init notebook to test the environment and initialize paths. License Project code is licensed under the MIT license . All other content is licensed under the Creative Commons Attribution 4.0 International license .","title":"Home"},{"location":"#literate-programming-and-interactive-reporting-with-jupyter-notebooks","text":"| Documentation | Dashboard Workshop materials for the 2022 Data Science Research Bazaar at UW-Madison. This is an intermediate level workshop that will teach how to use Jupyter notebooks with Python for literate programming and interactive presentation of research results. I walk participants through steps of a mini research project of visualizing geospatial data, focusing on tools and methods of making results accessible, reproducible and interactive. This workshop will use economic, demographic and geographic data characterizing US communities that are freely and publicly available from the US Census Bureau website. Prior programming and data analysis experience is recommended for full participation in the workshop. If you are new to Python, I recommend reading about Jupyter and pandas . This book shows how to use Jupyter notebooks for teaching and learning, and QuantEcon lectures use Python for economics and finance and are also a good resource for beginners. Workshop topics: - Jupyter notebooks and Markdown - code organization, Python modules and packages, conversion to scripts - static documentation, conversion to HTML - data retrieval via download and API - data processing and analysis in pandas - plotting with matplotlib - building interactive interfaces with notebook widgets - visualization of geospatial data, geopandas and ipyleaflet - hosting live notebooks and Voil\u00e0 dashboards on Binder","title":"Literate programming and interactive reporting with Jupyter notebooks"},{"location":"#setup","text":"You need a running Jupyter server in order to work with workshop notebooks. The easiest way is to launch a free cloud instance in Binder . A more difficult (but potentially more reliable) alternative is to create conda Python environment on your local computer.","title":"Setup"},{"location":"#using-binder","text":"Click this link to launch a new Binder instance and connect to it from your browser, then open and run the init notebook to test the environment and initialize paths. Ideal launch time is under 30 seconds, but it might take longer if the repository has been recently updated, because Binder will need to rebuild the environment from scratch. Notice that Binder platform provides computational resources for free, and so limitations are in place and availability can not be guaranteed. Read here about usage policy and available resources. Also static HTML documentation site will not be available.","title":"Using Binder"},{"location":"#local-python","text":"This method requires some experience or readiness to read documentation. As reward, you will have persistent environment under your control that does not depend on cloud service availability. This is also a typical way to set up Python for data work. Download and install latest miniconda , following instructions for your operating system. Skip this step if you already have conda command line utility available in your terminal. Open terminal (Anaconda Prompt on Windows) and clone this repository in a folder of your choice ( git clone https://github.com/antonbabkin/workshop-notebooks.git ). Alternatively, download and unpack repository code as ZIP. If you want to practice building and hosting HTML documentation, you need to use a repository that you control. One way to do it is to fork this repository and then clone your copy. In the terminal, navigate to the repository folder and create new conda environment . Environment specification will be read from the environment.yml file, all required packages will be downloaded and installed. cd workshop-notebooks conda env create Activate the environment and start JupyterLab server. This will start a new Jupyter server and open Jupyter interface in browser window. conda activate workshop-nbs-2022 jupyter lab In Jupyter, open and run the init notebook to test the environment and initialize paths.","title":"Local Python"},{"location":"#license","text":"Project code is licensed under the MIT license . All other content is licensed under the Creative Commons Attribution 4.0 International license .","title":"License"},{"location":"analysis/","text":"Analysis of population and employment dynamics In this module we will combine economic, demographic and geographic data to explore patterns of population and employment dynamics across states and counties. Main dataframes are stored in a global dict DF . During interactive notebook execution it is populated as needed. If imported as a module, function prep_data() should be called before using other module functions. DF = {} def prep_data(): DF['geo'] = data.geo() DF['by year'] = data_by_year() In data_by_year() we simply merge employment and population dataframes available from popemp.data module. DF['by year'] = data_by_year() DF['by year'].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty year pop pop_gr emp emp_gr 0 00 000 1990 249470539 NaN 93983875.0 2.145 1 00 000 1991 252208537 1.097524 91781210.0 -2.439 2 00 000 1992 255104027 1.148054 91752935.0 0.004 3 00 000 1993 257857622 1.079401 93252746.0 1.560 4 00 000 1994 260401091 0.986385 95712240.0 2.254 Dynamics in a given geographic area Function plot_growth(st, cty, y0, y1) makes a plot with population and employment dynamic in a chosen geographic area. Level series are normalized to 100 in base year y0 . Year-to-year growth rates are shown in a separate panel. For states and counties we also add lines for a bigger reference geography: states are compared to entire country, counties are compared to their state. We can see that Wisconsin population was growing slower than national, and that post-recession employment recovery was also slower. plot_growth('55', '000', 2005, 2015) Widgets Jupyter widgets are like other Python objects such as strings, lists or pandas dataframes. Like other objects widgets also store their state, have methods to do something useful with that state and have a representation suitable for rich rendering in a HTML view of a Jupyter notebook. Additional feature of widgets is that their visual representation can be updated dynamically and they can respond to user interaction. Here is a simple slider. We can read it's value in code from another cell and also change it's value programmatically. w = widgets.IntSlider(value=4, min=0, max=10, description='How many?') w IntSlider(value=4, description='How many?', max=10) print('He says', w.value) He says 4 w.value = 5 print('Now he says', w.value) Now he says 5 We can combine multiple widgets and make them do something useful together. A button here will add up two numbers and display result in a separate label widget. We can even be fancy and use \\LaTeX in text labels. # create widgets wx = widgets.IntSlider(2, 0, 5, description='$x$') wy = widgets.IntSlider(2, 0, 5, description='$y$') wb = widgets.Button(description='Add') wz = widgets.Label('$x + y = ?$') # useful function def how_many(x, y): z = x + y return 5 def click_handler(*args): # \"*args\" captures arguments passed from calling widget, but we ignore them here x = wx.value y = wy.value z = how_many(x, y) wz.value = f'${x} + {y} = {z}$' # run handler to fill initial values click_handler() # register handler with button widget wb.on_click(click_handler) # display widgets in a simple vertical layout widgets.VBox([wx, wy, wb, wz]) VBox(children=(IntSlider(value=2, description='$x$', max=5), IntSlider(value=2, description='$y$', max=5), But\u2026 Function st_cty_selectors() creates two dropdown widgets that can be used to select state and county using their names instead of codes, while codes are used internally to work with our dataframes. Lists of states and counties are populated from our global tables. Additional logic, wrapping inside of the function, updates list of counties dynamically every time the state is changed. We can now create a pair of linked widgets anywhere we need them later. Compare different areas We will now turn to comparing diffent areas in a cross-section. Function compute_agr(y0, y1) calculates average annual growth rate of population and employment in every area between y0 and y1 . Average growth rate of variable x_t between years s and t is calculated as x_{agr} = \\left(\\frac{x_t}{x_s}\\right)^{\\frac{1}{t-s+1}} . Every area is also labelled according as pop+ emp+ , pop+ emp- , pop- emp+ and pop- emp- using two growth measures: absolute percentage growth and relative to reference geographic area. color_from_agr_cat(df, abs_rel) returns a column of HEX color codes useful for plotting. d = compute_agr(2000, 2010) d['c'] = color_from_agr_cat(d, 'abs') d.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty emp_agr_abs pop_agr_abs ref_emp_agr ref_pop_agr pop_agr_rel emp_agr_rel agr_cat_abs agr_cat_rel c 0 00 000 -0.173731 0.838944 -0.173731 0.838944 0.000000 0.000000 pop+ emp- pop+ emp+ #1b9e77 1 01 000 -0.497312 0.658386 -0.173731 0.838944 -0.180558 -0.323581 pop+ emp- pop- emp- #1b9e77 2 01 001 0.546082 2.006507 -0.497312 0.658386 1.348121 1.043393 pop+ emp+ pop+ emp+ #d95f02 3 01 003 0.865040 2.381697 -0.497312 0.658386 1.723312 1.362352 pop+ emp+ pop+ emp+ #d95f02 4 01 005 -3.554504 -0.543406 -0.497312 0.658386 -1.201792 -3.057192 pop- emp- pop- emp- #7570b3 As with dynamics plot, plot_agr(st, y0, y1, abs_rel) can be used to generate figures with state or county average growth rates as a scatterplot. plot_agr('55', 2005, 2015, 'rel') Map Python package ipyleaflet is a wrapper around Leaflet.js and can generate customizable maps. Map objects are also Jupyter widgets, and so we can mix and match them with all other widgets and layout. It is helpful to wrap map widget in a class Map that stores map state and exposes interaction via click_callback and upd() methods.","title":"Analysis"},{"location":"analysis/#analysis-of-population-and-employment-dynamics","text":"In this module we will combine economic, demographic and geographic data to explore patterns of population and employment dynamics across states and counties. Main dataframes are stored in a global dict DF . During interactive notebook execution it is populated as needed. If imported as a module, function prep_data() should be called before using other module functions. DF = {} def prep_data(): DF['geo'] = data.geo() DF['by year'] = data_by_year() In data_by_year() we simply merge employment and population dataframes available from popemp.data module. DF['by year'] = data_by_year() DF['by year'].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty year pop pop_gr emp emp_gr 0 00 000 1990 249470539 NaN 93983875.0 2.145 1 00 000 1991 252208537 1.097524 91781210.0 -2.439 2 00 000 1992 255104027 1.148054 91752935.0 0.004 3 00 000 1993 257857622 1.079401 93252746.0 1.560 4 00 000 1994 260401091 0.986385 95712240.0 2.254","title":"Analysis of population and employment dynamics"},{"location":"analysis/#dynamics-in-a-given-geographic-area","text":"Function plot_growth(st, cty, y0, y1) makes a plot with population and employment dynamic in a chosen geographic area. Level series are normalized to 100 in base year y0 . Year-to-year growth rates are shown in a separate panel. For states and counties we also add lines for a bigger reference geography: states are compared to entire country, counties are compared to their state. We can see that Wisconsin population was growing slower than national, and that post-recession employment recovery was also slower. plot_growth('55', '000', 2005, 2015)","title":"Dynamics in a given geographic area"},{"location":"analysis/#widgets","text":"Jupyter widgets are like other Python objects such as strings, lists or pandas dataframes. Like other objects widgets also store their state, have methods to do something useful with that state and have a representation suitable for rich rendering in a HTML view of a Jupyter notebook. Additional feature of widgets is that their visual representation can be updated dynamically and they can respond to user interaction. Here is a simple slider. We can read it's value in code from another cell and also change it's value programmatically. w = widgets.IntSlider(value=4, min=0, max=10, description='How many?') w IntSlider(value=4, description='How many?', max=10) print('He says', w.value) He says 4 w.value = 5 print('Now he says', w.value) Now he says 5 We can combine multiple widgets and make them do something useful together. A button here will add up two numbers and display result in a separate label widget. We can even be fancy and use \\LaTeX in text labels. # create widgets wx = widgets.IntSlider(2, 0, 5, description='$x$') wy = widgets.IntSlider(2, 0, 5, description='$y$') wb = widgets.Button(description='Add') wz = widgets.Label('$x + y = ?$') # useful function def how_many(x, y): z = x + y return 5 def click_handler(*args): # \"*args\" captures arguments passed from calling widget, but we ignore them here x = wx.value y = wy.value z = how_many(x, y) wz.value = f'${x} + {y} = {z}$' # run handler to fill initial values click_handler() # register handler with button widget wb.on_click(click_handler) # display widgets in a simple vertical layout widgets.VBox([wx, wy, wb, wz]) VBox(children=(IntSlider(value=2, description='$x$', max=5), IntSlider(value=2, description='$y$', max=5), But\u2026 Function st_cty_selectors() creates two dropdown widgets that can be used to select state and county using their names instead of codes, while codes are used internally to work with our dataframes. Lists of states and counties are populated from our global tables. Additional logic, wrapping inside of the function, updates list of counties dynamically every time the state is changed. We can now create a pair of linked widgets anywhere we need them later.","title":"Widgets"},{"location":"analysis/#compare-different-areas","text":"We will now turn to comparing diffent areas in a cross-section. Function compute_agr(y0, y1) calculates average annual growth rate of population and employment in every area between y0 and y1 . Average growth rate of variable x_t between years s and t is calculated as x_{agr} = \\left(\\frac{x_t}{x_s}\\right)^{\\frac{1}{t-s+1}} . Every area is also labelled according as pop+ emp+ , pop+ emp- , pop- emp+ and pop- emp- using two growth measures: absolute percentage growth and relative to reference geographic area. color_from_agr_cat(df, abs_rel) returns a column of HEX color codes useful for plotting. d = compute_agr(2000, 2010) d['c'] = color_from_agr_cat(d, 'abs') d.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty emp_agr_abs pop_agr_abs ref_emp_agr ref_pop_agr pop_agr_rel emp_agr_rel agr_cat_abs agr_cat_rel c 0 00 000 -0.173731 0.838944 -0.173731 0.838944 0.000000 0.000000 pop+ emp- pop+ emp+ #1b9e77 1 01 000 -0.497312 0.658386 -0.173731 0.838944 -0.180558 -0.323581 pop+ emp- pop- emp- #1b9e77 2 01 001 0.546082 2.006507 -0.497312 0.658386 1.348121 1.043393 pop+ emp+ pop+ emp+ #d95f02 3 01 003 0.865040 2.381697 -0.497312 0.658386 1.723312 1.362352 pop+ emp+ pop+ emp+ #d95f02 4 01 005 -3.554504 -0.543406 -0.497312 0.658386 -1.201792 -3.057192 pop- emp- pop- emp- #7570b3 As with dynamics plot, plot_agr(st, y0, y1, abs_rel) can be used to generate figures with state or county average growth rates as a scatterplot. plot_agr('55', 2005, 2015, 'rel')","title":"Compare different areas"},{"location":"analysis/#map","text":"Python package ipyleaflet is a wrapper around Leaflet.js and can generate customizable maps. Map objects are also Jupyter widgets, and so we can mix and match them with all other widgets and layout. It is helpful to wrap map widget in a class Map that stores map state and exposes interaction via click_callback and upd() methods.","title":"Map"},{"location":"data/","text":"Retrieve and prepare data In this module we download, process and store geographic shapes, population and employment data from US Census Bureau. Geography We need state and county FIPS codes and names, and their shapes for map visualizations. Here we use 2018 Cartographic Boundary Files - simplified representations of selected geographic areas from the Census Bureau\u2019s MAF/TIGER geographic database. These boundary files are specifically designed for small scale thematic mapping. Function geo() downloads state and county 1:20,000,000 shapefiles using geopandas library, reshapes and combines them into single a GeoDataFrame. We use county code \"000\" as indicator of state rows. Resulting dataframe is cached on disk as a binary pickle file, and when subsequent calls of geo() will simply read and return the dataframe from cache to save time and avoid work. Delete data/geo.pkl if you want to re-create the dataframe, for example, after you changed the function. Similarly, download_file() also caches files on disk. This is the top of the dataframe. geo().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty name geometry 0 01 000 Alabama POLYGON ((-88.46866 31.89386, -88.46866 31.933... 1 01 001 Autauga county, Alabama POLYGON ((-86.91759 32.66417, -86.71339 32.661... 2 01 003 Baldwin county, Alabama POLYGON ((-88.02632 30.75336, -87.94455 30.827... 3 01 005 Barbour county, Alabama POLYGON ((-85.73573 31.62449, -85.66565 31.786... 4 01 007 Bibb county, Alabama POLYGON ((-87.42194 33.00338, -87.31854 33.006... geopandas stores shapes as shapely polygons in the geometry column. You can perform various geometric operations with these objects, refer to geopandas and shapely documentation. For example, let's select and plot all states that cross the band between -120 and -110 degrees of longitude, roughly US Pacific coast. d = geo().cx[-120:-110, :].query('cty == \"000\"') d.plot(); Be mindful of Coordinate Reference System ( CRS ) when working with shapefiles. If you combine shapefiles from multiple sources, make sure to align their CRS's. Census shapefiles come in EPSG:4269 . The same map in \"Spherical Mercator\" ( EPSG:3857 , used in Google Maps) will look like this. d.to_crs(epsg=3857).plot(); Population We are using annual state and county population 1990-2019 from Census Population Estimates Program ( PEP ). Data are available in 10 year blocks for 2010-2019 , 2000-2010 and 1990-2019 . Note on character encoding of plain text files, including CSV: newer files use \"UTF-8\" , older use \"ISO-8859-1\" . Post-2000 files are simple CSV tables. Functions pop_2010_2019() and pop_2000_2009() download and read them into dataframes with minor manipulation. 1990-1999 data are in a long text file. pop_1990_1990() does some more elaborate parsing. Table with state and county population has \"1\" as the first character in every line. We use this to read necessary lines into a temporary string buffer, and then parse the buffer into a dataframe. Finally, in pop() we call the three above functions to create three frames, combine them and add aggregated rows of national totals with state code \"00\" and county code \"000\" . We also compute year-to-year growth rate in percentage points in column pop_gr . Final dataframe is pickled for easy access. pop().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty year pop pop_gr 0 00 000 1990 249470539 NaN 1 00 000 1991 252208537 1.097524 2 00 000 1992 255104027 1.148054 3 00 000 1993 257857622 1.079401 4 00 000 1994 260401091 0.986385 Quick visual inspection of the data reveals an abnormal population jump between 1999 and 2000. It is clear on national and state level, but not so on county level. I could not find out the cause, but it is most likely a data artifact. This is something to be aware of, but it does not matter for the purposes of this project. d = pop().set_index('year') fig, ax = plt.subplots(1, 3, figsize=(16, 4)) d.query('st == \"00\" and cty == \"000\"')['pop'].plot(ax=ax[0]) ax[0].set_title('National') d.query('st == \"55\" and cty == \"000\"')['pop'].plot(ax=ax[1]) ax[1].set_title('Wisconsin') d.query('st == \"55\" and cty == \"025\"')['pop'].plot(ax=ax[2]) ax[2].set_title('Wisconsin, Dane county'); Employment State and county employment comes from Census Business Dynamics Statistics ( BDS ). This product has some improvements over more widely used County Business Patterns, and entire history can be downloaded in a single table from here . Data does not require much processing which is done in the emp() . National, state and county tables are downloaded and combined, again using convention of setting state to \"00\" for national and county to \"000\" for national and state rows. Percentage year-to-year growth rate is renamed from net_job_creation_rate to emp_gr . Data goes back to 1978, but we only need from 1990 for combination with population. emp().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty year emp emp_gr 0 00 000 1990 93983875.0 2.145 1 00 000 1991 91781210.0 -2.439 2 00 000 1992 91752935.0 0.004 3 00 000 1993 93252746.0 1.560 4 00 000 1994 95712240.0 2.254 d = emp().set_index('year') fig, ax = plt.subplots(1, 3, figsize=(16, 4)) d.query('st == \"00\" and cty == \"000\"')['emp'].plot(ax=ax[0]) ax[0].set_title('National') d.query('st == \"55\" and cty == \"000\"')['emp'].plot(ax=ax[1]) ax[1].set_title('Wisconsin') d.query('st == \"55\" and cty == \"025\"')['emp'].plot(ax=ax[2]) ax[2].set_title('Wisconsin, Dane county'); API Here is a little demo of retrieving a table from a data provider using API. We are not going to use it in this project, because bulk data download as readily available as CSV files, and API access rates are often limited and may require access key. However for some other data sources API access may be the only option. Another good use case is when whole data is huge, and you are building a web app (dashboard) that only needs to pull small pieces of data at a time. Here I show how to query a portion of the BDS dataset from Census Bureau API . Most of Census data products can be accessed like this, and BDS specific documentation is here with some query examples there . Typically, to use an API you need to submit a HTTP request and receive back a response. Request queries are customized by chanding parameters of the URL string, and responses return data in JSON, XML or some other format. Python requests library hides a lot of technical details and is easy to use. When you are constructing your query URL, you can also just open it in a browser for a quick preview. Here is a query line that will pull employment data for all counties in Wisconsin from 2015 to 2019. https://api.census.gov/data/timeseries/bds?get=NAME,ESTAB,EMP,YEAR&for=county:*&in=state:55&time=from+2015+to+2019&NAICS=00&key=YOUR_KEY_GOES_HERE Everything to the left of ? is the base URL or endpoint: https://api.census.gov/data/timeseries/bds . Everything to the right are key-value parameter pairs, separated by & : get=NAME,ESTAB,EMP,YEAR data columns for=county:* all counties in=state:55 state FIPS code \"55\" for Wisconsin time=from+2015+to+2019 time series range NAICS=00 \"00\" for economy-wide employment key=YOUR_KEY_GOES_HERE drop this part if you don't have a key Query limits: You can include up to 50 variables in a single API query and can make up to 500 queries per IP address per day. More than 500 queries per IP address per day requires that you register for a Census key. That key will be part of your data request URL string. Querying without a key will probably work for you, unless you are sharing your IP with many other users. You can obtain a key for free, but you should keep it secret and not accidentally share, for example, by hard-coding it in your code or commiting a file. Here I have my key in a text file that is ignored in .gitignore and only exists in my local copy of the repo. Another common appoach is to store keys in OS environment variables. import requests p = nbd.root/'census_api_key.txt' if p.exists(): key = '&key=' + p.read_text() else: key = '' base_url = 'https://api.census.gov/data/timeseries/bds' st = '55' y0, y1 = 2015, 2019 response = requests.get(f'{base_url}?get=NAME,ESTAB,EMP,YEAR&for=county:*&in=state:{st}&time=from+{y0}+to+{y1}&NAICS=00{key}') response_body = response.json() response_body[:5] [['NAME', 'ESTAB', 'EMP', 'YEAR', 'time', 'NAICS', 'state', 'county'], ['Iron County, Wisconsin', '176', '1360', '2015', '2015', '00', '55', '051'], ['Iron County, Wisconsin', '172', '1416', '2016', '2016', '00', '55', '051'], ['Iron County, Wisconsin', '172', '1337', '2017', '2017', '00', '55', '051'], ['Iron County, Wisconsin', '166', '1362', '2018', '2018', '00', '55', '051']] df = pd.DataFrame(response_body[1:], columns=response_body[0]) df.query('county == \"025\"').head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } NAME ESTAB EMP YEAR time NAICS state county 246 Dane County, Wisconsin 12649 269895 2015 2015 00 55 025 247 Dane County, Wisconsin 12974 275642 2016 2016 00 55 025 248 Dane County, Wisconsin 13047 280403 2017 2017 00 55 025 249 Dane County, Wisconsin 13088 297744 2018 2018 00 55 025 250 Dane County, Wisconsin 13099 301777 2019 2019 00 55 025","title":"Data"},{"location":"data/#retrieve-and-prepare-data","text":"In this module we download, process and store geographic shapes, population and employment data from US Census Bureau.","title":"Retrieve and prepare data"},{"location":"data/#geography","text":"We need state and county FIPS codes and names, and their shapes for map visualizations. Here we use 2018 Cartographic Boundary Files - simplified representations of selected geographic areas from the Census Bureau\u2019s MAF/TIGER geographic database. These boundary files are specifically designed for small scale thematic mapping. Function geo() downloads state and county 1:20,000,000 shapefiles using geopandas library, reshapes and combines them into single a GeoDataFrame. We use county code \"000\" as indicator of state rows. Resulting dataframe is cached on disk as a binary pickle file, and when subsequent calls of geo() will simply read and return the dataframe from cache to save time and avoid work. Delete data/geo.pkl if you want to re-create the dataframe, for example, after you changed the function. Similarly, download_file() also caches files on disk. This is the top of the dataframe. geo().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty name geometry 0 01 000 Alabama POLYGON ((-88.46866 31.89386, -88.46866 31.933... 1 01 001 Autauga county, Alabama POLYGON ((-86.91759 32.66417, -86.71339 32.661... 2 01 003 Baldwin county, Alabama POLYGON ((-88.02632 30.75336, -87.94455 30.827... 3 01 005 Barbour county, Alabama POLYGON ((-85.73573 31.62449, -85.66565 31.786... 4 01 007 Bibb county, Alabama POLYGON ((-87.42194 33.00338, -87.31854 33.006... geopandas stores shapes as shapely polygons in the geometry column. You can perform various geometric operations with these objects, refer to geopandas and shapely documentation. For example, let's select and plot all states that cross the band between -120 and -110 degrees of longitude, roughly US Pacific coast. d = geo().cx[-120:-110, :].query('cty == \"000\"') d.plot(); Be mindful of Coordinate Reference System ( CRS ) when working with shapefiles. If you combine shapefiles from multiple sources, make sure to align their CRS's. Census shapefiles come in EPSG:4269 . The same map in \"Spherical Mercator\" ( EPSG:3857 , used in Google Maps) will look like this. d.to_crs(epsg=3857).plot();","title":"Geography"},{"location":"data/#population","text":"We are using annual state and county population 1990-2019 from Census Population Estimates Program ( PEP ). Data are available in 10 year blocks for 2010-2019 , 2000-2010 and 1990-2019 . Note on character encoding of plain text files, including CSV: newer files use \"UTF-8\" , older use \"ISO-8859-1\" . Post-2000 files are simple CSV tables. Functions pop_2010_2019() and pop_2000_2009() download and read them into dataframes with minor manipulation. 1990-1999 data are in a long text file. pop_1990_1990() does some more elaborate parsing. Table with state and county population has \"1\" as the first character in every line. We use this to read necessary lines into a temporary string buffer, and then parse the buffer into a dataframe. Finally, in pop() we call the three above functions to create three frames, combine them and add aggregated rows of national totals with state code \"00\" and county code \"000\" . We also compute year-to-year growth rate in percentage points in column pop_gr . Final dataframe is pickled for easy access. pop().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty year pop pop_gr 0 00 000 1990 249470539 NaN 1 00 000 1991 252208537 1.097524 2 00 000 1992 255104027 1.148054 3 00 000 1993 257857622 1.079401 4 00 000 1994 260401091 0.986385 Quick visual inspection of the data reveals an abnormal population jump between 1999 and 2000. It is clear on national and state level, but not so on county level. I could not find out the cause, but it is most likely a data artifact. This is something to be aware of, but it does not matter for the purposes of this project. d = pop().set_index('year') fig, ax = plt.subplots(1, 3, figsize=(16, 4)) d.query('st == \"00\" and cty == \"000\"')['pop'].plot(ax=ax[0]) ax[0].set_title('National') d.query('st == \"55\" and cty == \"000\"')['pop'].plot(ax=ax[1]) ax[1].set_title('Wisconsin') d.query('st == \"55\" and cty == \"025\"')['pop'].plot(ax=ax[2]) ax[2].set_title('Wisconsin, Dane county');","title":"Population"},{"location":"data/#employment","text":"State and county employment comes from Census Business Dynamics Statistics ( BDS ). This product has some improvements over more widely used County Business Patterns, and entire history can be downloaded in a single table from here . Data does not require much processing which is done in the emp() . National, state and county tables are downloaded and combined, again using convention of setting state to \"00\" for national and county to \"000\" for national and state rows. Percentage year-to-year growth rate is renamed from net_job_creation_rate to emp_gr . Data goes back to 1978, but we only need from 1990 for combination with population. emp().head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } st cty year emp emp_gr 0 00 000 1990 93983875.0 2.145 1 00 000 1991 91781210.0 -2.439 2 00 000 1992 91752935.0 0.004 3 00 000 1993 93252746.0 1.560 4 00 000 1994 95712240.0 2.254 d = emp().set_index('year') fig, ax = plt.subplots(1, 3, figsize=(16, 4)) d.query('st == \"00\" and cty == \"000\"')['emp'].plot(ax=ax[0]) ax[0].set_title('National') d.query('st == \"55\" and cty == \"000\"')['emp'].plot(ax=ax[1]) ax[1].set_title('Wisconsin') d.query('st == \"55\" and cty == \"025\"')['emp'].plot(ax=ax[2]) ax[2].set_title('Wisconsin, Dane county');","title":"Employment"},{"location":"data/#api","text":"Here is a little demo of retrieving a table from a data provider using API. We are not going to use it in this project, because bulk data download as readily available as CSV files, and API access rates are often limited and may require access key. However for some other data sources API access may be the only option. Another good use case is when whole data is huge, and you are building a web app (dashboard) that only needs to pull small pieces of data at a time. Here I show how to query a portion of the BDS dataset from Census Bureau API . Most of Census data products can be accessed like this, and BDS specific documentation is here with some query examples there . Typically, to use an API you need to submit a HTTP request and receive back a response. Request queries are customized by chanding parameters of the URL string, and responses return data in JSON, XML or some other format. Python requests library hides a lot of technical details and is easy to use. When you are constructing your query URL, you can also just open it in a browser for a quick preview. Here is a query line that will pull employment data for all counties in Wisconsin from 2015 to 2019. https://api.census.gov/data/timeseries/bds?get=NAME,ESTAB,EMP,YEAR&for=county:*&in=state:55&time=from+2015+to+2019&NAICS=00&key=YOUR_KEY_GOES_HERE Everything to the left of ? is the base URL or endpoint: https://api.census.gov/data/timeseries/bds . Everything to the right are key-value parameter pairs, separated by & : get=NAME,ESTAB,EMP,YEAR data columns for=county:* all counties in=state:55 state FIPS code \"55\" for Wisconsin time=from+2015+to+2019 time series range NAICS=00 \"00\" for economy-wide employment key=YOUR_KEY_GOES_HERE drop this part if you don't have a key Query limits: You can include up to 50 variables in a single API query and can make up to 500 queries per IP address per day. More than 500 queries per IP address per day requires that you register for a Census key. That key will be part of your data request URL string. Querying without a key will probably work for you, unless you are sharing your IP with many other users. You can obtain a key for free, but you should keep it secret and not accidentally share, for example, by hard-coding it in your code or commiting a file. Here I have my key in a text file that is ignored in .gitignore and only exists in my local copy of the repo. Another common appoach is to store keys in OS environment variables. import requests p = nbd.root/'census_api_key.txt' if p.exists(): key = '&key=' + p.read_text() else: key = '' base_url = 'https://api.census.gov/data/timeseries/bds' st = '55' y0, y1 = 2015, 2019 response = requests.get(f'{base_url}?get=NAME,ESTAB,EMP,YEAR&for=county:*&in=state:{st}&time=from+{y0}+to+{y1}&NAICS=00{key}') response_body = response.json() response_body[:5] [['NAME', 'ESTAB', 'EMP', 'YEAR', 'time', 'NAICS', 'state', 'county'], ['Iron County, Wisconsin', '176', '1360', '2015', '2015', '00', '55', '051'], ['Iron County, Wisconsin', '172', '1416', '2016', '2016', '00', '55', '051'], ['Iron County, Wisconsin', '172', '1337', '2017', '2017', '00', '55', '051'], ['Iron County, Wisconsin', '166', '1362', '2018', '2018', '00', '55', '051']] df = pd.DataFrame(response_body[1:], columns=response_body[0]) df.query('county == \"025\"').head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } NAME ESTAB EMP YEAR time NAICS state county 246 Dane County, Wisconsin 12649 269895 2015 2015 00 55 025 247 Dane County, Wisconsin 12974 275642 2016 2016 00 55 025 248 Dane County, Wisconsin 13047 280403 2017 2017 00 55 025 249 Dane County, Wisconsin 13088 297744 2018 2018 00 55 025 250 Dane County, Wisconsin 13099 301777 2019 2019 00 55 025","title":"API"},{"location":"tools/","text":"Tools and utilities This notebook contains general purpose tools that are used throughout the project and might also be useful outside. NBD: development in the notebook This section defines the Nbd class that can be used to convert a notebook into a script or a documentation page. This approach was greatly inspired by the nbdev project and can be thought of as a reduced and simplified nbdev . To use Nbd , create an instance with the name of your package and call methods from that instance like so: nbd = Nbd('popemp') . Method Nbd.nb2mod() selectively exports code cells into a script, making it easily importable in other parts of the project and also leaving out scratch and exploratory code. To mark code cell for export to a module, put #nbd module in the first line of the cell. All imports from project modules into notebooks should take absolute form from ... import ... , they will be automatically converted to relative import in scripts. For example, from popemp.tools import Nbd will become from .tools import Nbd . Turning notebook into a documentation page takes two steps. First, notebook is converted to a Markdown file in docs_src/ folder, and then all Markdown files are turned into a searchable HTML website in docs/ folder using MkDocs utility. To mark code cell for documentation, put #nbd docs in the first line of the cell. It can also be #nbd module docs if cell goes both to module and documentation. To mark markdown cells, add [nbd]: # \"docs\" in the first line of the cell. Method Nbd.build_docs() performs both steps for each notebook specified in the mkdocs.yml file at project root. This file can be also used to configure site structure. Site is stored in docs/ folder and pushed to GitHub, so it can be easily hosted as GitHub pages (enable in GitHub repo settings). Check MkDocs documentation for configuration options and other deployment alternatives. Example and testing of Nbd . nbd = Nbd('popemp') print(f'Project root directory: \"{nbd.root}\"') Project root directory: \"/Users/anton/work/workshop-notebooks\" Notebooks and Git Jupyter notebooks are technically JSON files with possibly embedded binary data in output cells (e.g. images). This make them not very Git friendly, because most Git tools are designed to work with plain text files. Git diffs of notebooks are not very readable, merges break notebooks, and binary blobs clog storage and don't diff or merge. Multiple approaches exist to address these problems, and we won't cover them here, but here are some options that you can try and see what works best for you. Manually clear output Before commiting a notebook to git, right click it in JupyterLab, select \"Clear All Outputs\" and save. This is a very simple approach that I have used in the past. It solves the issue of embedded binary data and some execution counters. Yet a lot of notebook and cell metadata will still remain. And sometimes you will forget to clear before committing. :) Automated clearing You can configure Git to automatically run clearing operations when you commit notebooks. Several tools can do this, for example nbclean , nbstripout , nbdev and even standard nbconvert that comes together with Jupyter installation. I am using nbconvert for clearing and nbdime for diffing and merging. You need to configure Git like this. nbdime config-git --enable git config filter.jupyternotebook.clean \"jupyter nbconvert --stdin --stdout --to=notebook --ClearOutputPreprocessor.enabled=True --ClearMetadataPreprocessor.enabled=True --log-level=ERROR\" git config filter.jupyternotebook.smudge cat git config filter.jupyternotebook.required true git config diff.jupyternotebook.command \"git-nbdiffdriver diff --ignore-outputs --ignore-metadata --ignore-details\" For these filters to apply, notebooks should be registered in .gitattributes file. *.ipynb filter=jupyternotebook *.ipynb diff=jupyternotebook *.ipynb merge=jupyternotebook Do not store notebooks in Git :) Well, not exactly. Notebooks are converted to plain text for Git storage and back for execution. This is an approach taken by the Jupytext project. I have not used this myself, but want to try at some point. Misc download_file() takes URL, downloads file to specified location and returns it's path. Subsequent calls to the same function return cached copy from disk. We use this function to automate manual operations, to simplify replication and to allow pulling data into our cloud-hosted dashboard. Example: download LICENSE file from GitHub repo and assert that it's contents is the same as in the local repo version. nbd = Nbd('popemp') f = download_file('https://raw.githubusercontent.com/antonbabkin/workshop-notebooks/main/LICENSE', nbd.root, 'LICENSE_COPY') assert open(nbd.root/'LICENSE').read() == open(f).read() f.unlink() Download complete: LICENSE_COPY. Build this module This notebook itself is turned into importable module by running the code below. nbd = Nbd('popemp') nbd.nb2mod('tools.ipynb') Converted notebook \"nbs/tools.ipynb\" to module \"popemp/tools.py\".","title":"Tools"},{"location":"tools/#tools-and-utilities","text":"This notebook contains general purpose tools that are used throughout the project and might also be useful outside.","title":"Tools and utilities"},{"location":"tools/#nbd-development-in-the-notebook","text":"This section defines the Nbd class that can be used to convert a notebook into a script or a documentation page. This approach was greatly inspired by the nbdev project and can be thought of as a reduced and simplified nbdev . To use Nbd , create an instance with the name of your package and call methods from that instance like so: nbd = Nbd('popemp') . Method Nbd.nb2mod() selectively exports code cells into a script, making it easily importable in other parts of the project and also leaving out scratch and exploratory code. To mark code cell for export to a module, put #nbd module in the first line of the cell. All imports from project modules into notebooks should take absolute form from ... import ... , they will be automatically converted to relative import in scripts. For example, from popemp.tools import Nbd will become from .tools import Nbd . Turning notebook into a documentation page takes two steps. First, notebook is converted to a Markdown file in docs_src/ folder, and then all Markdown files are turned into a searchable HTML website in docs/ folder using MkDocs utility. To mark code cell for documentation, put #nbd docs in the first line of the cell. It can also be #nbd module docs if cell goes both to module and documentation. To mark markdown cells, add [nbd]: # \"docs\" in the first line of the cell. Method Nbd.build_docs() performs both steps for each notebook specified in the mkdocs.yml file at project root. This file can be also used to configure site structure. Site is stored in docs/ folder and pushed to GitHub, so it can be easily hosted as GitHub pages (enable in GitHub repo settings). Check MkDocs documentation for configuration options and other deployment alternatives. Example and testing of Nbd . nbd = Nbd('popemp') print(f'Project root directory: \"{nbd.root}\"') Project root directory: \"/Users/anton/work/workshop-notebooks\"","title":"NBD: development in the notebook"},{"location":"tools/#notebooks-and-git","text":"Jupyter notebooks are technically JSON files with possibly embedded binary data in output cells (e.g. images). This make them not very Git friendly, because most Git tools are designed to work with plain text files. Git diffs of notebooks are not very readable, merges break notebooks, and binary blobs clog storage and don't diff or merge. Multiple approaches exist to address these problems, and we won't cover them here, but here are some options that you can try and see what works best for you.","title":"Notebooks and Git"},{"location":"tools/#manually-clear-output","text":"Before commiting a notebook to git, right click it in JupyterLab, select \"Clear All Outputs\" and save. This is a very simple approach that I have used in the past. It solves the issue of embedded binary data and some execution counters. Yet a lot of notebook and cell metadata will still remain. And sometimes you will forget to clear before committing. :)","title":"Manually clear output"},{"location":"tools/#automated-clearing","text":"You can configure Git to automatically run clearing operations when you commit notebooks. Several tools can do this, for example nbclean , nbstripout , nbdev and even standard nbconvert that comes together with Jupyter installation. I am using nbconvert for clearing and nbdime for diffing and merging. You need to configure Git like this. nbdime config-git --enable git config filter.jupyternotebook.clean \"jupyter nbconvert --stdin --stdout --to=notebook --ClearOutputPreprocessor.enabled=True --ClearMetadataPreprocessor.enabled=True --log-level=ERROR\" git config filter.jupyternotebook.smudge cat git config filter.jupyternotebook.required true git config diff.jupyternotebook.command \"git-nbdiffdriver diff --ignore-outputs --ignore-metadata --ignore-details\" For these filters to apply, notebooks should be registered in .gitattributes file. *.ipynb filter=jupyternotebook *.ipynb diff=jupyternotebook *.ipynb merge=jupyternotebook","title":"Automated clearing"},{"location":"tools/#do-not-store-notebooks-in-git","text":"Well, not exactly. Notebooks are converted to plain text for Git storage and back for execution. This is an approach taken by the Jupytext project. I have not used this myself, but want to try at some point.","title":"Do not store notebooks in Git :)"},{"location":"tools/#misc","text":"download_file() takes URL, downloads file to specified location and returns it's path. Subsequent calls to the same function return cached copy from disk. We use this function to automate manual operations, to simplify replication and to allow pulling data into our cloud-hosted dashboard. Example: download LICENSE file from GitHub repo and assert that it's contents is the same as in the local repo version. nbd = Nbd('popemp') f = download_file('https://raw.githubusercontent.com/antonbabkin/workshop-notebooks/main/LICENSE', nbd.root, 'LICENSE_COPY') assert open(nbd.root/'LICENSE').read() == open(f).read() f.unlink() Download complete: LICENSE_COPY.","title":"Misc"},{"location":"tools/#build-this-module","text":"This notebook itself is turned into importable module by running the code below. nbd = Nbd('popemp') nbd.nb2mod('tools.ipynb') Converted notebook \"nbs/tools.ipynb\" to module \"popemp/tools.py\".","title":"Build this module"}]}